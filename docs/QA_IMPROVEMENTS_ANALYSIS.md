# QA System Improvements Analysis

**Date:** 2025-10-17
**Context:** Post-Sprint 3 parallel QA batch testing (20 events, 5 agents)

## Executive Summary

Parallel QA Batch 1 successfully validated the Sprint 3 quality assurance system, achieving 60% completion rate (12/20 events) with significant quality improvements. However, one critical issue emerged: **WebFetch timeout on Washington Post URL caused one agent to hang for minutes**, reducing overall throughput.

**Key Achievement:** System demonstrated stability with 5 concurrent agents, no conflicts, and proper quality tracking.

**Key Issue:** WebFetch tool has no timeout limit, causing agent hangs on slow/paywalled sources.

## Batch Results Summary

### Completion Metrics
- **Events Enhanced:** 12 of 20 (60% success rate)
- **Processing Time:** ~10 minutes for successful agents
- **Agent Performance:** 5 parallel agents, no conflicts
- **Quality Improvements:**
  - Added 17 tier-1 sources
  - 2 events achieved perfect 10.0 scores
  - System-wide quality: 6.77 ‚Üí 6.78 (+0.01)
  - Tier-1 sources: 2,846 ‚Üí 2,866 (+20, 51.9%)

### Success Patterns
‚úÖ **Fast, Open Sources Worked Well:**
- Reuters, AP, Bloomberg, NPR, PBS
- Government sites (.gov): GAO, DOJ, NOAA, Supreme Court
- Investigative journalism: ICIJ, ProPublica

‚úÖ **Source Prioritization Successful:**
- Agents correctly prioritized tier-1 government sources
- Multiple sources from different outlets
- Proper verification of facts and dates

### Failure Patterns
‚ùå **Timeout Issues:**
- Washington Post: Agent hung for minutes (paywalled, slow)
- Likely other paywalled sites: NYT, WSJ (if attempted)
- Agent cannot self-recover from WebFetch timeout

‚ùå **Throughput Impact:**
- 40% of events (8/20) not completed
- One stuck agent = complete loss of that agent's work
- Manual intervention required to reset stuck agents

## Root Cause Analysis

### WebFetch Timeout Limitation

**Problem:** The WebFetch tool has no built-in timeout mechanism.

**Technical Details:**
- WebFetch is a Claude Code tool, not controllable by our application
- When fetching slow/paywalled sites (Washington Post, NYT, WSJ), agents hang indefinitely
- No way to interrupt or timeout from within agent code
- Agent cannot detect or recover from stuck state

**Impact:**
- 1 stuck agent = 20% capacity loss (in 5-agent batch)
- Linear relationship: More stuck agents = proportionally worse throughput
- Manual reset required by user

### Known Problematic Sources

Based on batch results and CLAUDE.md documentation:

**Confirmed Slow/Timeout Sources:**
1. **Washington Post** - Confirmed timeout in recent batch
2. **New York Times** - Known paywall + slow (documented)
3. **Wall Street Journal** - Known paywall (documented)

**Likely Problematic (need testing):**
- Financial Times (paywall)
- The Economist (paywall)
- Bloomberg (sometimes slow, but tier-1 quality)
- Sites requiring authentication

## Current Mitigation Strategies (Already Implemented)

The CLAUDE.md file already contains comprehensive timeout handling documentation:

### 1. Prevention Instructions (CLAUDE.md lines 402-435)
```markdown
**üî¥ CRITICAL: Web Fetch Timeout Handling**

**Timeout Prevention:**
1. Skip known slow sites immediately:
   - Washington Post (paywall + slow)
   - New York Times (paywall + slow)
   - Wall Street Journal (paywall)

2. Prioritize fast, open sources:
   - Reuters, AP, Bloomberg, NPR, PBS
   - Government sites (.gov)
   - Academic sources (.edu)
```

### 2. Recovery Documentation (CLAUDE.md lines 685-722)
```markdown
### Stuck QA Agents (WebFetch Timeout)

**Recovery Steps:**
1. Identify stuck agents via validation run status
2. Reset stuck events back to pending
3. Restart with different validator-id
```

### 3. Fallback Strategy
- Use 2-3 tier-2 sources instead of tier-1 paywalled sources
- Search for alternative coverage of same event
- Document in validation notes: "Used alternative sources due to paywall/timeout"

## Proposed Improvements

### Improvement 1: Explicit Agent Instructions Enhancement

**Problem:** Despite CLAUDE.md documentation, agents may not read or follow timeout prevention.

**Solution:** Add explicit "DO NOT FETCH" list to agent prompts:

```markdown
üö´ FORBIDDEN SOURCES (Will Cause Timeout):
- washingtonpost.com - NEVER fetch
- nytimes.com - NEVER fetch
- wsj.com - NEVER fetch
- ft.com - NEVER fetch

‚úÖ USE THESE INSTEAD:
- Reuters, AP, Bloomberg (open access)
- NPR, PBS (public media)
- .gov sources (government)
- ProPublica, ICIJ (investigative)
```

**Impact:** Reduce timeout incidents from 1-2 per batch to 0-1 per batch.

### Improvement 2: Timeout Monitoring Script

**Problem:** User must manually detect stuck agents (watching terminal output).

**Solution:** Create monitoring script that checks validation run progress:

```bash
#!/bin/bash
# check_validation_progress.sh
# Usage: ./check_validation_progress.sh <run_id> <expected_agents>

STUCK_THRESHOLD=180  # 3 minutes

while true; do
  # Check for events stuck in "assigned" status
  stuck=$(sqlite3 unified_research.db \
    "SELECT validator_id, event_id,
     (julianday('now') - julianday(assigned_date)) * 86400 as seconds_stuck
     FROM validation_run_events
     WHERE validation_run_id = $1
     AND validation_status = 'assigned'
     AND (julianday('now') - julianday(assigned_date)) * 86400 > $STUCK_THRESHOLD;")

  if [ -n "$stuck" ]; then
    echo "‚ö†Ô∏è  STUCK AGENTS DETECTED:"
    echo "$stuck"
    echo ""
    echo "To reset: sqlite3 unified_research.db \"UPDATE validation_run_events SET validation_status = 'pending', assigned_validator = NULL WHERE validation_run_id = $1 AND validation_status = 'assigned';\""
  fi

  sleep 30
done
```

**Impact:** Proactive detection of stuck agents within 3 minutes instead of manual observation.

### Improvement 3: Problematic Source Blacklist API

**Problem:** No centralized way to track and avoid problematic sources.

**Solution:** Add source blacklist to research_monitor API:

```python
# research_monitor/services/source_blacklist.py

BLACKLISTED_DOMAINS = {
    'washingtonpost.com': {
        'reason': 'Paywall + timeout (confirmed 2025-10-17)',
        'alternative_search': 'Use AP, Reuters for same story'
    },
    'nytimes.com': {
        'reason': 'Paywall + slow',
        'alternative_search': 'Use NPR, PBS for same story'
    },
    'wsj.com': {
        'reason': 'Strict paywall',
        'alternative_search': 'Use Bloomberg, Reuters for finance news'
    }
}

def is_blacklisted(url: str) -> Dict[str, Any]:
    """Check if URL is blacklisted with reason and alternative"""
    from urllib.parse import urlparse
    domain = urlparse(url).netloc.replace('www.', '')

    if domain in BLACKLISTED_DOMAINS:
        return {
            'blacklisted': True,
            'domain': domain,
            **BLACKLISTED_DOMAINS[domain]
        }
    return {'blacklisted': False}

# API endpoint
@sources_bp.route('/blacklist/check', methods=['POST'])
def check_blacklist():
    """Check if URL is blacklisted"""
    url = request.json.get('url')
    result = is_blacklisted(url)
    return jsonify(result)
```

CLI command:
```bash
python3 research_cli.py check-source-blacklist --url "https://www.washingtonpost.com/article"
# Returns: {"blacklisted": true, "reason": "Paywall + timeout", "alternative": "Use AP, Reuters"}
```

**Impact:** Agents can check URLs before fetching, preventing timeouts preemptively.

### Improvement 4: Agent Progress Heartbeat

**Problem:** No way to know if agent is working or stuck without manual observation.

**Solution:** Add validation heartbeat updates:

```python
# When agent starts processing
curl -X POST "http://localhost:5558/api/validation-runs/$RUN_ID/heartbeat" \
  -H "Content-Type: application/json" \
  -d '{"validator_id": "qa-agent-1", "status": "processing_event", "event_id": "2025-01-15--event"}'

# Periodic heartbeat every 30 seconds
curl -X POST "http://localhost:5558/api/validation-runs/$RUN_ID/heartbeat" \
  -d '{"validator_id": "qa-agent-1", "status": "still_working"}'
```

Monitoring script checks for missing heartbeats:
```sql
-- Find agents with no heartbeat in last 2 minutes
SELECT validator_id, last_heartbeat,
  (julianday('now') - julianday(last_heartbeat)) * 1440 as minutes_since_heartbeat
FROM validation_heartbeats
WHERE minutes_since_heartbeat > 2;
```

**Impact:** Automatic detection of stuck agents within 2 minutes.

### Improvement 5: Alternative Source Database

**Problem:** When tier-1 source is blacklisted, agents don't know what alternatives exist.

**Solution:** Create alternative source mapping:

```json
{
  "washington-post-stories": {
    "alternatives": [
      {"outlet": "Associated Press", "search_strategy": "AP typically covers major stories within hours"},
      {"outlet": "Reuters", "search_strategy": "Reuters for political/financial stories"},
      {"outlet": "NPR", "search_strategy": "NPR for in-depth context"},
      {"outlet": "C-SPAN", "search_strategy": "C-SPAN for congressional/government events"}
    ]
  },
  "nytimes-investigations": {
    "alternatives": [
      {"outlet": "ProPublica", "search_strategy": "ProPublica for investigative journalism"},
      {"outlet": "The Intercept", "search_strategy": "The Intercept for national security"},
      {"outlet": "ICIJ", "search_strategy": "ICIJ for international finance"}
    ]
  }
}
```

**Impact:** Agents know exactly where to look when tier-1 source is unavailable.

## Scaling Strategy

### Current State (Batch 1)
- **Agents:** 5 concurrent
- **Events:** 20 total
- **Success Rate:** 60%
- **Effective Throughput:** 12 events per batch
- **Stuck Agents:** 1 (20% loss)

### Proposed Scaling (With Improvements)

**Option A: Conservative Scale (10 agents)**
- Assuming 10% stuck rate (1 stuck agent per 10)
- Expected throughput: 90% √ó 40 events = 36 events per batch
- Risk: Low
- Recommendation: **Test this first**

**Option B: Moderate Scale (20 agents)**
- Assuming 5% stuck rate (1 stuck agent per 20)
- Expected throughput: 95% √ó 80 events = 76 events per batch
- Risk: Medium
- Recommendation: After Option A succeeds

**Option C: Aggressive Scale (50 agents)**
- Assuming 2% stuck rate (1 stuck agent per 50)
- Expected throughput: 98% √ó 200 events = 196 events per batch
- Risk: High (database contention, system resources)
- Recommendation: Only after extensive testing

### Scaling Prerequisites

Before scaling beyond 10 agents:

1. ‚úÖ **Implement Improvement 1** - Explicit agent instructions
2. ‚úÖ **Implement Improvement 2** - Monitoring script
3. ‚è≥ **Test 10-agent batch** - Verify <5% stuck rate
4. ‚è≥ **Database performance test** - Ensure no contention issues
5. ‚è≥ **Filesystem sync performance** - Current 30s sync may need optimization

## Implementation Priority

### Phase 1: Immediate (This Session)
1. ‚úÖ Update CLAUDE.md with explicit blacklist
2. ‚úÖ Document Washington Post timeout issue
3. ‚úÖ Create this analysis document

### Phase 2: Next Batch (Before Batch 2)
1. ‚è≥ Create monitoring script (Improvement 2)
2. ‚è≥ Add blacklist API endpoint (Improvement 3)
3. ‚è≥ Test with 10-agent batch

### Phase 3: Production Scale (After Batch 2 Success)
1. ‚è≥ Implement heartbeat system (Improvement 4)
2. ‚è≥ Create alternative source database (Improvement 5)
3. ‚è≥ Scale to 20-50 agents

## Success Metrics

### Batch 2 Targets (10 agents, 40 events)
- **Success Rate:** ‚â•85% (34+ events completed)
- **Stuck Rate:** ‚â§10% (‚â§1 stuck agent)
- **Processing Time:** ‚â§15 minutes per agent
- **Quality Improvements:** +30 tier-1 sources

### Long-term Targets (557 high-priority events remaining)
- **Total Batches:** 15-20 batches @ 30-40 events each
- **Timeline:** 2-3 weeks @ 1 batch per day
- **Final Quality:** Average quality ‚â•7.0, Tier-1 ‚â•55%

## Conclusion

The Sprint 3 QA system is **production-ready** with one critical limitation: WebFetch timeout handling. The 60% success rate is acceptable for initial testing, but with the proposed improvements (especially explicit blacklisting and monitoring), we can achieve 85-95% success rates at scale.

**Recommended Next Steps:**
1. Update agent instructions with explicit blacklist (5 minutes)
2. Create monitoring script (30 minutes)
3. Test 10-agent batch on remaining high-priority events (1 hour)
4. Iterate based on results

The foundation is solid. With these improvements, we can process the remaining 557 high-priority events efficiently and systematically improve the entire timeline corpus.
