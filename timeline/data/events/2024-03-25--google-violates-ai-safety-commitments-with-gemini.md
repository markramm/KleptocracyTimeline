---
id: 2024-03-25--google-violates-ai-safety-commitments-with-gemini
date: '2024-03-25'
title: UK Lawmakers Accuse Google of Breaking AI Safety Pledge with Gemini 2.5 Pro
  Release
importance: 8
actors:
- Google DeepMind
- Baroness Beeban Kidron
- Des Browne
- UK AI Safety Institute
tags:
- ai-safety
- regulatory-violations
- tech-accountability
- ai-governance
- transparency-failures
- frontier-ai-safety
sources:
- outlet: TIME Magazine
  title: 60 U.K. Lawmakers Accuse Google of Breaking AI Safety Pledge
  url: https://time.com/7313320/google-deepmind-gemini-ai-safety-pledge/
  date: '2025-08-29'
- outlet: Government of the United Kingdom
  title: AI Safety Institute Overview
  url: https://www.gov.uk/government/publications/ai-safety-institute-overview/introducing-the-ai-safety-institute
  date: '2024-02-01'
- outlet: Fortune
  title: British Lawmakers Accuse Google DeepMind of 'Breach of Trust' Over Delayed
    Gemini 2.5 Pro Safety Report
  url: https://fortune.com/2025/08/29/british-lawmakers-accuse-google-deepmind-of-breach-of-trust-over-delayed-gemini-2-5-pro-safety-report/
  date: '2025-08-29'
- outlet: MLex
  title: Google DeepMind Accused of Breaking AI Safety Commitments by UK Lawmakers
  url: https://www.mlex.com/mlex/artificial-intelligence/articles/2382751/google-deepmind-accused-of-breaking-ai-safety-commitments-by-uk-lawmakers
  date: '2024-03-25'
capture_lanes:
- Regulatory Capture
- Tech Governance
---

Sixty U.K. lawmakers accused Google DeepMind of violating international AI safety commitments by releasing Gemini 2.5 Pro without comprehensive public safety disclosures. The allegations center on Google's failure to 'publicly report' system capabilities and risk assessments as pledged at a February 2024 international AI summit co-hosted by the U.K. and South Korea.

Key concerns include:
- Releasing the model without detailed safety information
- Not immediately clarifying external testing processes
- Treating safety commitments as optional rather than mandatory

Notable signatories like Baroness Beeban Kidron and former Defence Secretary Des Browne warned that such practices could trigger a dangerous precedent in AI development.

The accusations highlight a broader industry trend where major AI companies appear to be retreating from comprehensive safety reporting, potentially undermining international AI governance efforts.
